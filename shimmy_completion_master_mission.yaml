version: "1.0"
name: "shimmy_completion_master_mission"
description: "Complete shimmy local-first AI serving architecture using RustChain missions"
steps:
  - id: "analyze_current_shimmy_state"
    name: "Analyze current shimmy implementation"
    step_type: "llm"
    parameters:
      prompt: "Based on your training data about my shimmy project, help me create a comprehensive analysis of the current state and what needs to be completed. I'm building a local-first AI serving solution that bridges cloud AI and enterprise compliance needs. What are the key areas that likely need work?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 500

  - id: "create_shimmy_analysis"
    name: "Save shimmy analysis"
    step_type: "create_file"
    depends_on: ["analyze_current_shimmy_state"]
    parameters:
      path: "../shimmy/CURRENT_STATE_ANALYSIS.md"
      content: "# Shimmy Current State Analysis\n\nGenerated by RustChain + Champion LLM\n\n## Analysis Results\n\n[Analysis content will be populated by LLM response]"

  - id: "plan_auto_discovery_implementation"
    name: "Plan auto-discovery features"
    step_type: "llm"
    parameters:
      prompt: "For shimmy's auto-discovery system, I need to implement filesystem scanning for models, format detection (GGUF, SafeTensors, ONNX), and automatic registry population. Based on your knowledge of my coding patterns, what's the best approach for this in Rust?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 400

  - id: "plan_rustchain_integration"
    name: "Plan RustChain integration strategy"
    step_type: "llm"
    parameters:
      prompt: "I want shimmy to serve as the local inference backend for RustChain's AI agent framework. How should I implement a ShimmyProvider that replaces the current OllamaProvider? What API compatibility considerations are important?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 400

  - id: "prioritize_next_steps"
    name: "Prioritize implementation steps"
    step_type: "llm"
    parameters:
      prompt: "Based on the analysis, auto-discovery planning, and RustChain integration needs, help me create a prioritized action plan. What should I implement first to get the most value quickly?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 300

config:
  max_parallel_steps: 1
  timeout_seconds: 120
