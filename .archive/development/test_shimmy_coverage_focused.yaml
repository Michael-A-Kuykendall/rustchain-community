name: "Focused Shimmy Coverage Test"
description: "Champion model creates proper Rust tests for shimmy_provider.rs to achieve 85% coverage"
version: "1.0"

steps:
  - id: "analyze_current_tests"
    name: "Analyze Current Test Coverage"
    step_type: "llm"
    parameters:
      provider: "ollama"
      model: "llama32-champion"
      prompt: |
        You are creating additional Rust unit tests for the shimmy_provider.rs file.
        
        Current situation:
        - File has 427 lines of code
        - Currently has 4 basic tests (creation, conversion methods)
        - Untested methods: complete(), stream(), list_models(), get_model_info()
        - Target: Add tests to reach 85% coverage
        
        The ShimmyProvider struct implements the LLMProvider trait with these methods:
        - async fn complete(&self, request: LLMRequest) -> Result<LLMResponse>
        - async fn stream(&self, request: LLMRequest) -> Result<StreamType>  
        - async fn list_models(&self) -> Result<Vec<ModelInfo>>
        - async fn get_model_info(&self, model_id: &str) -> Result<ModelInfo>
        
        Create exactly 6 additional test methods that:
        1. Test HTTP success scenarios with mocked responses
        2. Test HTTP error scenarios (404, 500, timeout)
        3. Test request/response conversion edge cases
        4. Use mockito or similar for HTTP mocking
        5. Follow Rust testing best practices
        6. Actually increase code coverage of untested methods
        
        Respond with ONLY the Rust test code, properly formatted, ready to add to the existing test module. No explanations, just clean compilable Rust code.
      temperature: 0.1
      max_tokens: 1500
      
  - id: "add_tests_to_shimmy_file"
    name: "Add Tests to Shimmy Provider"
    step_type: "create_file"
    parameters:
      path: "shimmy_additional_tests.rs" 
      content: |
        // Additional tests for shimmy_provider.rs
        // Generated by Champion model to achieve 85%+ coverage
        
        {analyze_current_tests}
    depends_on: ["analyze_current_tests"]