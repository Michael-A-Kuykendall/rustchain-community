version: "1.0"
name: "shimmy_complete_integration_stack"
description: "Systematic integration of all shimmy components using Champion LLM analysis"

steps:
  - id: "analyze_current_warnings"
    name: "Analyze compiler warnings and unused code"
    step_type: "llm"
    parameters:
      prompt: "I have a Rust project 'shimmy' with 21 compiler warnings about unused code. The main issues are: 1) unused function 'handle_list_command_with_discovery' in cli.rs, 2) unused RustChainRequest/Response types, 3) unused MetricsCollector system, 4) unused SafeTensors adapter. These indicate features that are built but not connected. How should I prioritize integrating these systems?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 500
      
  - id: "create_integration_plan"
    name: "Create detailed integration roadmap"
    step_type: "create_file"
    depends_on: ["analyze_current_warnings"]
    parameters:
      path: "../shimmy/INTEGRATION_ROADMAP.md"
      content: "# Shimmy Integration Roadmap\n\nGenerated by RustChain Champion Analysis\n\n## Priority Integration Tasks\n\n[Analysis from LLM will guide this content]\n\n## Implementation Steps\n\n1. Connect Discovery to CLI\n2. Wire RustChain compatibility\n3. Integrate metrics system\n4. Connect SafeTensors adapter\n5. Production readiness\n\n## Success Criteria\n\n- All 21 warnings resolved\n- Discovery system functional\n- RustChain provider working\n- Metrics operational\n"
      
  - id: "plan_discovery_cli_integration"
    name: "Plan discovery-CLI integration"
    step_type: "llm"
    parameters:
      prompt: "For shimmy's CLI module, I need to connect the ModelDiscovery system to the 'list' command. Currently there's an unused function 'handle_list_command_with_discovery'. How should I modify the CLI to use actual filesystem discovery instead of hardcoded model registry?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 400
      
  - id: "plan_rustchain_integration"
    name: "Plan RustChain compatibility integration"
    step_type: "llm"
    parameters:
      prompt: "I have RustChain compatibility types (RustChainRequest/Response) and a rustchain_generate function that are unused. How should I add RustChain-compatible endpoints to shimmy's axum server to make it work as a drop-in replacement for Ollama in RustChain workflows?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 400
      
  - id: "plan_metrics_integration"
    name: "Plan metrics system integration"
    step_type: "llm"
    parameters:
      prompt: "I have a complete MetricsCollector system but it's unused. How should I integrate metrics collection into shimmy's API endpoints and add a /metrics endpoint for monitoring token generation, request counts, and performance?"
      model: "llama32-champion"
      temperature: 0.3
      max_tokens: 400
      
  - id: "save_integration_plans"
    name: "Save all integration plans"
    step_type: "create_file"
    depends_on: ["plan_discovery_cli_integration", "plan_rustchain_integration", "plan_metrics_integration"]
    parameters:
      path: "../shimmy/DETAILED_INTEGRATION_PLANS.md"
      content: "# Detailed Integration Plans for Shimmy\n\nGenerated by Champion LLM analysis of current codebase\n\n## Discovery-CLI Integration Plan\n\n[Details from discovery integration analysis]\n\n## RustChain Integration Plan\n\n[Details from RustChain integration analysis]\n\n## Metrics Integration Plan\n\n[Details from metrics integration analysis]\n\n## Implementation Order\n\n1. Discovery system first (foundation)\n2. RustChain compatibility (core feature)\n3. Metrics system (monitoring)\n4. Production readiness (final polish)\n"

config:
  max_parallel_steps: 1
  timeout_seconds: 300
  
llm_config:
  model: "llama32-champion"
  temperature: 0.3
  max_tokens: 500
